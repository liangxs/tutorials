
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "advanced/numpy_extensions_tutorial.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_advanced_numpy_extensions_tutorial.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_advanced_numpy_extensions_tutorial.py:


Creating Extensions Using NumPy and SciPy
=========================================
**Author**: `Adam Paszke <https://github.com/apaszke>`_

**Updated by**: `Adam Dziedzic <https://github.com/adam-dziedzic>`_

In this tutorial, we shall go through two tasks:

1. Create a neural network layer with no parameters.

    -  This calls into **numpy** as part of its implementation

2. Create a neural network layer that has learnable weights

    -  This calls into **SciPy** as part of its implementation

.. GENERATED FROM PYTHON SOURCE LINES 19-23

.. code-block:: Python


    import torch
    from torch.autograd import Function








.. GENERATED FROM PYTHON SOURCE LINES 24-33

Parameter-less example
----------------------

This layer doesnâ€™t particularly do anything useful or mathematically
correct.

It is aptly named ``BadFFTFunction``

**Layer Implementation**

.. GENERATED FROM PYTHON SOURCE LINES 33-57

.. code-block:: Python


    from numpy.fft import rfft2, irfft2


    class BadFFTFunction(Function):
        @staticmethod
        def forward(ctx, input):
            numpy_input = input.detach().numpy()
            result = abs(rfft2(numpy_input))
            return input.new(result)

        @staticmethod
        def backward(ctx, grad_output):
            numpy_go = grad_output.numpy()
            result = irfft2(numpy_go)
            return grad_output.new(result)

    # since this layer does not have any parameters, we can
    # simply declare this as a function, rather than as an ``nn.Module`` class


    def incorrect_fft(input):
        return BadFFTFunction.apply(input)








.. GENERATED FROM PYTHON SOURCE LINES 58-59

**Example usage of the created layer:**

.. GENERATED FROM PYTHON SOURCE LINES 59-66

.. code-block:: Python


    input = torch.randn(8, 8, requires_grad=True)
    result = incorrect_fft(input)
    print(result)
    result.backward(torch.randn(result.size()))
    print(input)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    tensor([[ 3.1581,  3.7880,  4.0245,  9.7965, 12.7429],
            [ 2.3007,  5.4496,  1.8765,  7.7742,  5.8454],
            [ 4.7550,  7.4864,  8.0402,  6.2824, 14.7260],
            [ 2.0530,  4.5374,  2.3095, 10.8985,  6.8253],
            [11.9568,  7.7237,  5.0125,  7.8252,  4.1949],
            [ 2.0530,  9.9669,  6.1489, 10.6216,  6.8253],
            [ 4.7550,  5.8787,  3.8711,  2.7210, 14.7260],
            [ 2.3007,  8.9419,  4.5497, 16.3240,  5.8454]],
           grad_fn=<BadFFTFunctionBackward>)
    tensor([[-0.4741,  0.9250, -2.0878, -1.1646, -0.0850,  0.7579,  0.1341,  1.1033],
            [-0.7232,  0.4598,  2.0030, -0.7349, -1.2464,  0.0149, -1.0968,  1.2027],
            [-0.0506, -0.2630,  1.5681, -0.8264, -0.3224, -0.7322, -1.0586, -0.0758],
            [ 0.5757,  0.9392,  0.8477, -0.8585,  0.5935,  0.8082, -0.3576,  0.9921],
            [ 0.4460, -1.5480, -0.4363,  0.6093, -0.9717,  1.6922, -0.9642,  0.1387],
            [-1.2642,  0.8971, -1.6880,  2.5743, -0.7296,  0.7756,  0.5517,  0.4212],
            [ 1.1266, -0.6065, -0.1727, -0.4113, -1.1339,  1.0214,  0.1459, -0.6829],
            [ 0.1523,  0.6206,  1.5767, -0.1160,  0.7400, -0.0245, -0.3906,  0.0414]],
           requires_grad=True)




.. GENERATED FROM PYTHON SOURCE LINES 67-79

Parametrized example
--------------------

In deep learning literature, this layer is confusingly referred
to as convolution while the actual operation is cross-correlation
(the only difference is that filter is flipped for convolution,
which is not the case for cross-correlation).

Implementation of a layer with learnable weights, where cross-correlation
has a filter (kernel) that represents weights.

The backward pass computes the gradient ``wrt`` the input and the gradient ``wrt`` the filter.

.. GENERATED FROM PYTHON SOURCE LINES 79-120

.. code-block:: Python


    from numpy import flip
    import numpy as np
    from scipy.signal import convolve2d, correlate2d
    from torch.nn.modules.module import Module
    from torch.nn.parameter import Parameter


    class ScipyConv2dFunction(Function):
        @staticmethod
        def forward(ctx, input, filter, bias):
            # detach so we can cast to NumPy
            input, filter, bias = input.detach(), filter.detach(), bias.detach()
            result = correlate2d(input.numpy(), filter.numpy(), mode='valid')
            result += bias.numpy()
            ctx.save_for_backward(input, filter, bias)
            return torch.as_tensor(result, dtype=input.dtype)

        @staticmethod
        def backward(ctx, grad_output):
            grad_output = grad_output.detach()
            input, filter, bias = ctx.saved_tensors
            grad_output = grad_output.numpy()
            grad_bias = np.sum(grad_output, keepdims=True)
            grad_input = convolve2d(grad_output, filter.numpy(), mode='full')
            # the previous line can be expressed equivalently as:
            # grad_input = correlate2d(grad_output, flip(flip(filter.numpy(), axis=0), axis=1), mode='full')
            grad_filter = correlate2d(input.numpy(), grad_output, mode='valid')
            return torch.from_numpy(grad_input), torch.from_numpy(grad_filter).to(torch.float), torch.from_numpy(grad_bias).to(torch.float)


    class ScipyConv2d(Module):
        def __init__(self, filter_width, filter_height):
            super(ScipyConv2d, self).__init__()
            self.filter = Parameter(torch.randn(filter_width, filter_height))
            self.bias = Parameter(torch.randn(1, 1))

        def forward(self, input):
            return ScipyConv2dFunction.apply(input, self.filter, self.bias)









.. GENERATED FROM PYTHON SOURCE LINES 121-122

**Example usage:**

.. GENERATED FROM PYTHON SOURCE LINES 122-131

.. code-block:: Python


    module = ScipyConv2d(3, 3)
    print("Filter and bias: ", list(module.parameters()))
    input = torch.randn(10, 10, requires_grad=True)
    output = module(input)
    print("Output from the convolution: ", output)
    output.backward(torch.randn(8, 8))
    print("Gradient for the input map: ", input.grad)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Filter and bias:  [Parameter containing:
    tensor([[ 1.0619,  0.5616, -1.9954],
            [-0.3027, -0.1683,  0.2291],
            [ 1.0464,  0.8310,  0.0622]], requires_grad=True), Parameter containing:
    tensor([[0.8960]], requires_grad=True)]
    Output from the convolution:  tensor([[-0.5952,  6.8563, -3.9529, -6.0334,  1.0813,  4.0910,  0.6688, -0.1266],
            [-1.8453, -1.7101,  2.7844,  2.4408,  3.4408,  2.2051, -0.9154,  3.4184],
            [ 5.2958,  1.6638,  3.7226, -4.7313, -0.9998,  2.1024, -1.5823,  0.7876],
            [-0.9186, -4.2902,  3.5014,  4.7435,  3.1716,  2.3963,  2.8333,  4.1455],
            [ 0.2518,  1.6537, -1.0396, -1.1083, -0.6338, -2.1722, -1.0151, -1.9811],
            [ 3.0808,  0.2368, -0.4322,  0.4226,  4.1296,  4.0539,  2.1026,  3.7085],
            [ 3.6411,  0.3747, -0.4262,  4.5705, -1.1357, -1.8623,  1.4950, -3.8056],
            [ 0.4306,  1.4612, -1.3083, -0.0632, -0.6635,  3.0500,  5.3418,  7.0971]],
           grad_fn=<ScipyConv2dFunctionBackward>)
    Gradient for the input map:  tensor([[-1.6465, -1.0967,  2.7881, -1.0052, -0.9036,  1.0875,  0.5855,  3.1270,
              0.2220, -1.7650],
            [ 2.5179,  0.7331, -6.0263, -0.1483,  2.2282,  2.2080,  1.9724,  1.0988,
             -0.2023, -4.1491],
            [-2.5048, -0.9719,  2.6504, -3.3810, -5.5758, -0.3094,  2.8239,  3.0600,
             -2.5575, -1.6576],
            [ 1.4439,  1.7880, -1.1313, -5.8768,  0.6683,  0.8694, -1.3905,  5.8922,
              1.9977, -0.8144],
            [ 1.1854,  1.1370, -1.4732, -1.0923, -1.6509,  0.4493,  1.4366,  0.6790,
              3.2448,  0.6274],
            [-3.4249, -1.1275,  2.7385, -1.6778,  4.3469, -2.0506, -0.9001,  3.1287,
             -1.0136,  0.2157],
            [ 1.3365,  0.6409,  0.6362, -0.4882, -1.4023,  2.6125, -2.4451, -1.5097,
              0.7176, -1.2972],
            [-1.2612, -3.1625, -5.6652, -0.2959, -1.0963,  0.4730, -0.4794,  1.8518,
              2.9072, -2.3415],
            [-0.8790, -0.8728, -0.0116, -0.8074,  0.3399,  0.7500,  0.3754,  0.2066,
              0.0225,  0.3238],
            [ 0.9194, -0.8708, -1.3625, -1.5356, -1.6003, -1.0143, -1.7056,  0.2605,
              0.9568,  0.0773]])




.. GENERATED FROM PYTHON SOURCE LINES 132-133

**Check the gradients:**

.. GENERATED FROM PYTHON SOURCE LINES 133-141

.. code-block:: Python


    from torch.autograd.gradcheck import gradcheck

    moduleConv = ScipyConv2d(3, 3)

    input = [torch.randn(20, 20, dtype=torch.double, requires_grad=True)]
    test = gradcheck(moduleConv, input, eps=1e-6, atol=1e-4)
    print("Are the gradients correct: ", test)




.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Are the gradients correct:  True





.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 0.592 seconds)


.. _sphx_glr_download_advanced_numpy_extensions_tutorial.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: numpy_extensions_tutorial.ipynb <numpy_extensions_tutorial.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: numpy_extensions_tutorial.py <numpy_extensions_tutorial.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: numpy_extensions_tutorial.zip <numpy_extensions_tutorial.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
