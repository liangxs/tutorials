
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "advanced/numpy_extensions_tutorial.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_advanced_numpy_extensions_tutorial.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_advanced_numpy_extensions_tutorial.py:


Creating Extensions Using NumPy and SciPy
=========================================
**Author**: `Adam Paszke <https://github.com/apaszke>`_

**Updated by**: `Adam Dziedzic <https://github.com/adam-dziedzic>`_

In this tutorial, we shall go through two tasks:

1. Create a neural network layer with no parameters.

    -  This calls into **numpy** as part of its implementation

2. Create a neural network layer that has learnable weights

    -  This calls into **SciPy** as part of its implementation

.. GENERATED FROM PYTHON SOURCE LINES 19-23

.. code-block:: default


    import torch
    from torch.autograd import Function








.. GENERATED FROM PYTHON SOURCE LINES 24-33

Parameter-less example
----------------------

This layer doesnâ€™t particularly do anything useful or mathematically
correct.

It is aptly named ``BadFFTFunction``

**Layer Implementation**

.. GENERATED FROM PYTHON SOURCE LINES 33-57

.. code-block:: default


    from numpy.fft import rfft2, irfft2


    class BadFFTFunction(Function):
        @staticmethod
        def forward(ctx, input):
            numpy_input = input.detach().numpy()
            result = abs(rfft2(numpy_input))
            return input.new(result)

        @staticmethod
        def backward(ctx, grad_output):
            numpy_go = grad_output.numpy()
            result = irfft2(numpy_go)
            return grad_output.new(result)

    # since this layer does not have any parameters, we can
    # simply declare this as a function, rather than as an ``nn.Module`` class


    def incorrect_fft(input):
        return BadFFTFunction.apply(input)








.. GENERATED FROM PYTHON SOURCE LINES 58-59

**Example usage of the created layer:**

.. GENERATED FROM PYTHON SOURCE LINES 59-66

.. code-block:: default


    input = torch.randn(8, 8, requires_grad=True)
    result = incorrect_fft(input)
    print(result)
    result.backward(torch.randn(result.size()))
    print(input)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    tensor([[ 6.5684, 12.8719, 14.4295,  3.7040, 10.7245],
            [ 4.7140, 17.2766,  2.0647, 10.2883,  9.7364],
            [ 5.1818,  8.8789,  0.3158,  7.6750,  6.7824],
            [10.9179,  3.5386,  4.3486,  2.1403, 10.4226],
            [ 4.7472,  3.5721,  3.0095,  5.4500,  2.8758],
            [10.9179,  2.4816,  4.1034, 12.0800, 10.4226],
            [ 5.1818,  9.0896,  9.4147,  6.6993,  6.7824],
            [ 4.7140,  4.8177,  1.2791,  2.9467,  9.7364]],
           grad_fn=<BadFFTFunctionBackward>)
    tensor([[-1.7469,  2.0675, -1.0724, -0.6086, -0.6736,  1.3157, -0.7911,  0.6256],
            [ 0.7977,  0.0899, -0.6858, -0.6436, -0.9355,  0.2998,  1.6321, -0.5482],
            [ 0.7832, -0.1073,  1.0758,  0.8462,  1.0896,  1.2350,  0.4860,  0.5173],
            [ 0.1658,  0.1973,  0.9681,  0.0030, -0.3898,  0.6980, -1.9852, -1.5625],
            [ 0.2663,  2.1006,  0.7064, -0.6536,  0.3850, -0.4089,  0.1437, -0.6452],
            [ 0.7420,  2.4588,  0.2878, -1.0274, -0.8536, -0.6673, -0.2203,  1.5745],
            [ 0.6456,  1.0354,  0.6573, -1.0757, -1.3087, -0.1343, -1.2173,  0.1194],
            [ 0.1741,  1.6957, -1.5676,  0.1270,  0.5461, -0.3746, -0.1827,  0.0970]],
           requires_grad=True)




.. GENERATED FROM PYTHON SOURCE LINES 67-79

Parametrized example
--------------------

In deep learning literature, this layer is confusingly referred
to as convolution while the actual operation is cross-correlation
(the only difference is that filter is flipped for convolution,
which is not the case for cross-correlation).

Implementation of a layer with learnable weights, where cross-correlation
has a filter (kernel) that represents weights.

The backward pass computes the gradient ``wrt`` the input and the gradient ``wrt`` the filter.

.. GENERATED FROM PYTHON SOURCE LINES 79-120

.. code-block:: default


    from numpy import flip
    import numpy as np
    from scipy.signal import convolve2d, correlate2d
    from torch.nn.modules.module import Module
    from torch.nn.parameter import Parameter


    class ScipyConv2dFunction(Function):
        @staticmethod
        def forward(ctx, input, filter, bias):
            # detach so we can cast to NumPy
            input, filter, bias = input.detach(), filter.detach(), bias.detach()
            result = correlate2d(input.numpy(), filter.numpy(), mode='valid')
            result += bias.numpy()
            ctx.save_for_backward(input, filter, bias)
            return torch.as_tensor(result, dtype=input.dtype)

        @staticmethod
        def backward(ctx, grad_output):
            grad_output = grad_output.detach()
            input, filter, bias = ctx.saved_tensors
            grad_output = grad_output.numpy()
            grad_bias = np.sum(grad_output, keepdims=True)
            grad_input = convolve2d(grad_output, filter.numpy(), mode='full')
            # the previous line can be expressed equivalently as:
            # grad_input = correlate2d(grad_output, flip(flip(filter.numpy(), axis=0), axis=1), mode='full')
            grad_filter = correlate2d(input.numpy(), grad_output, mode='valid')
            return torch.from_numpy(grad_input), torch.from_numpy(grad_filter).to(torch.float), torch.from_numpy(grad_bias).to(torch.float)


    class ScipyConv2d(Module):
        def __init__(self, filter_width, filter_height):
            super(ScipyConv2d, self).__init__()
            self.filter = Parameter(torch.randn(filter_width, filter_height))
            self.bias = Parameter(torch.randn(1, 1))

        def forward(self, input):
            return ScipyConv2dFunction.apply(input, self.filter, self.bias)









.. GENERATED FROM PYTHON SOURCE LINES 121-122

**Example usage:**

.. GENERATED FROM PYTHON SOURCE LINES 122-131

.. code-block:: default


    module = ScipyConv2d(3, 3)
    print("Filter and bias: ", list(module.parameters()))
    input = torch.randn(10, 10, requires_grad=True)
    output = module(input)
    print("Output from the convolution: ", output)
    output.backward(torch.randn(8, 8))
    print("Gradient for the input map: ", input.grad)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Filter and bias:  [Parameter containing:
    tensor([[-1.2042,  0.8023,  0.2834],
            [ 0.9733,  0.1488,  0.4449],
            [-1.5918,  0.6464, -0.0360]], requires_grad=True), Parameter containing:
    tensor([[0.6734]], requires_grad=True)]
    Output from the convolution:  tensor([[-2.1500, -0.7313,  0.7474, -0.7352, -2.9483,  1.9423,  3.3204, -1.3789],
            [ 3.1314,  1.4372, -1.0405, -3.8884,  9.4405,  0.8548,  1.6449,  2.7808],
            [-1.8877,  0.4852,  4.9772,  3.7222, -6.3123,  3.3261,  1.1035, -3.8751],
            [ 1.6658,  1.9985, -2.2783, -4.2469,  3.7546,  3.2301, -1.0953,  4.1662],
            [-1.7457, -0.4057, -0.8768,  3.6584, -2.4459, -0.1851,  5.5705, -2.6750],
            [ 0.5135,  2.3559,  2.4786, -1.0469,  2.0268,  2.4916,  1.0614,  3.5403],
            [ 2.6302,  5.4690, -0.5494,  1.9125, -0.9083,  1.7484, -0.6665,  0.8087],
            [-1.8034,  0.7492,  3.9903, -1.0640,  0.1755, -0.2756,  1.1645, -2.1586]],
           grad_fn=<ScipyConv2dFunctionBackward>)
    Gradient for the input map:  tensor([[ 1.2167, -3.3287,  0.7069,  0.1342,  1.0154, -1.6276, -0.3671, -0.5162,
              1.5774,  0.4384],
            [-2.6492,  1.5045,  1.5669,  4.4398, -0.3729,  1.6584, -0.6210,  3.3132,
              0.5767,  0.5434],
            [ 2.6007, -4.8476,  0.6210, -0.6314, -0.9475, -3.1736, -0.0089, -1.5229,
              0.8283, -0.4485],
            [-3.4713,  3.1338,  0.9321,  6.4031, -0.0854,  4.3349, -0.9556,  1.6515,
             -0.5808, -0.3532],
            [ 2.3924, -4.9310, -0.2037, -1.5921,  2.7447, -1.1031, -1.6733,  1.0134,
              0.9980,  0.1009],
            [-4.4495,  2.1819, -1.6174,  1.5430, -8.3282,  2.6918, -0.2020,  3.8267,
              1.0061,  0.3474],
            [ 2.4066, -0.8489,  0.0349,  0.0984,  3.2101,  2.5443,  0.3556, -0.4989,
             -0.0551, -0.3927],
            [-0.2426, -0.0343,  4.0243, -0.1160, -2.2418, -3.3346, -4.5672, -0.6207,
             -0.2859, -0.1798],
            [-1.3776, -0.9863, -1.3573,  0.1240, -2.7014,  2.7563,  2.2344,  1.3430,
             -0.0232,  0.3619],
            [ 0.9566, -0.5911,  1.2271,  1.1837,  0.6928, -1.0209, -1.1281, -0.6359,
              0.4481, -0.0267]])




.. GENERATED FROM PYTHON SOURCE LINES 132-133

**Check the gradients:**

.. GENERATED FROM PYTHON SOURCE LINES 133-141

.. code-block:: default


    from torch.autograd.gradcheck import gradcheck

    moduleConv = ScipyConv2d(3, 3)

    input = [torch.randn(20, 20, dtype=torch.double, requires_grad=True)]
    test = gradcheck(moduleConv, input, eps=1e-6, atol=1e-4)
    print("Are the gradients correct: ", test)




.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Are the gradients correct:  True





.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  0.585 seconds)


.. _sphx_glr_download_advanced_numpy_extensions_tutorial.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example


    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: numpy_extensions_tutorial.py <numpy_extensions_tutorial.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: numpy_extensions_tutorial.ipynb <numpy_extensions_tutorial.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
