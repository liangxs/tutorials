
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "advanced/numpy_extensions_tutorial.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_advanced_numpy_extensions_tutorial.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_advanced_numpy_extensions_tutorial.py:


Creating Extensions Using NumPy and SciPy
=========================================
**Author**: `Adam Paszke <https://github.com/apaszke>`_

**Updated by**: `Adam Dziedzic <https://github.com/adam-dziedzic>`_

In this tutorial, we shall go through two tasks:

1. Create a neural network layer with no parameters.

    -  This calls into **numpy** as part of its implementation

2. Create a neural network layer that has learnable weights

    -  This calls into **SciPy** as part of its implementation

.. GENERATED FROM PYTHON SOURCE LINES 19-23

.. code-block:: Python


    import torch
    from torch.autograd import Function








.. GENERATED FROM PYTHON SOURCE LINES 24-33

Parameter-less example
----------------------

This layer doesnâ€™t particularly do anything useful or mathematically
correct.

It is aptly named ``BadFFTFunction``

**Layer Implementation**

.. GENERATED FROM PYTHON SOURCE LINES 33-57

.. code-block:: Python


    from numpy.fft import rfft2, irfft2


    class BadFFTFunction(Function):
        @staticmethod
        def forward(ctx, input):
            numpy_input = input.detach().numpy()
            result = abs(rfft2(numpy_input))
            return input.new(result)

        @staticmethod
        def backward(ctx, grad_output):
            numpy_go = grad_output.numpy()
            result = irfft2(numpy_go)
            return grad_output.new(result)

    # since this layer does not have any parameters, we can
    # simply declare this as a function, rather than as an ``nn.Module`` class


    def incorrect_fft(input):
        return BadFFTFunction.apply(input)








.. GENERATED FROM PYTHON SOURCE LINES 58-59

**Example usage of the created layer:**

.. GENERATED FROM PYTHON SOURCE LINES 59-66

.. code-block:: Python


    input = torch.randn(8, 8, requires_grad=True)
    result = incorrect_fft(input)
    print(result)
    result.backward(torch.randn(result.size()))
    print(input)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    tensor([[ 2.8430, 12.1244,  0.5543,  4.8844,  2.3255],
            [ 6.6325,  4.5347,  7.2523,  9.8478,  4.2549],
            [ 6.6081,  5.5519,  6.8929,  7.4141,  4.6635],
            [ 8.7964,  9.1802,  5.0087,  5.6622,  3.6889],
            [ 2.1572,  9.0806,  6.2893,  2.9434,  6.1254],
            [ 8.7964,  9.2587,  7.3159, 11.4386,  3.6889],
            [ 6.6081,  7.1463,  4.0717,  4.7128,  4.6635],
            [ 6.6325,  6.6263,  3.9072,  8.6163,  4.2549]],
           grad_fn=<BadFFTFunctionBackward>)
    tensor([[-0.3501, -1.1009,  0.2507,  0.1749,  2.4163,  1.6817,  0.2498, -0.1310],
            [-0.3225, -0.9049,  0.4930, -0.9238,  0.2737, -1.1889, -1.7044,  1.2549],
            [-2.0357, -1.1272,  0.1489, -0.3299,  0.8503,  0.0287,  0.7330, -0.6152],
            [-0.8807, -0.0090,  0.4574,  0.0453,  0.2076,  2.0657, -0.3888,  0.5443],
            [ 0.8026,  0.2972,  0.6672,  0.2445, -0.7880, -0.0603, -0.0286,  0.2233],
            [ 0.3677,  1.6486, -0.1822, -0.0776,  1.0407,  0.0039,  0.7140, -0.5035],
            [-0.7505, -1.4150,  0.7706,  0.8927, -0.1281,  0.8864,  0.5544, -0.5123],
            [ 0.4682, -0.4525, -1.0472,  0.8582, -0.1565, -0.2228, -0.1185, -1.0165]],
           requires_grad=True)




.. GENERATED FROM PYTHON SOURCE LINES 67-79

Parametrized example
--------------------

In deep learning literature, this layer is confusingly referred
to as convolution while the actual operation is cross-correlation
(the only difference is that filter is flipped for convolution,
which is not the case for cross-correlation).

Implementation of a layer with learnable weights, where cross-correlation
has a filter (kernel) that represents weights.

The backward pass computes the gradient ``wrt`` the input and the gradient ``wrt`` the filter.

.. GENERATED FROM PYTHON SOURCE LINES 79-120

.. code-block:: Python


    from numpy import flip
    import numpy as np
    from scipy.signal import convolve2d, correlate2d
    from torch.nn.modules.module import Module
    from torch.nn.parameter import Parameter


    class ScipyConv2dFunction(Function):
        @staticmethod
        def forward(ctx, input, filter, bias):
            # detach so we can cast to NumPy
            input, filter, bias = input.detach(), filter.detach(), bias.detach()
            result = correlate2d(input.numpy(), filter.numpy(), mode='valid')
            result += bias.numpy()
            ctx.save_for_backward(input, filter, bias)
            return torch.as_tensor(result, dtype=input.dtype)

        @staticmethod
        def backward(ctx, grad_output):
            grad_output = grad_output.detach()
            input, filter, bias = ctx.saved_tensors
            grad_output = grad_output.numpy()
            grad_bias = np.sum(grad_output, keepdims=True)
            grad_input = convolve2d(grad_output, filter.numpy(), mode='full')
            # the previous line can be expressed equivalently as:
            # grad_input = correlate2d(grad_output, flip(flip(filter.numpy(), axis=0), axis=1), mode='full')
            grad_filter = correlate2d(input.numpy(), grad_output, mode='valid')
            return torch.from_numpy(grad_input), torch.from_numpy(grad_filter).to(torch.float), torch.from_numpy(grad_bias).to(torch.float)


    class ScipyConv2d(Module):
        def __init__(self, filter_width, filter_height):
            super(ScipyConv2d, self).__init__()
            self.filter = Parameter(torch.randn(filter_width, filter_height))
            self.bias = Parameter(torch.randn(1, 1))

        def forward(self, input):
            return ScipyConv2dFunction.apply(input, self.filter, self.bias)









.. GENERATED FROM PYTHON SOURCE LINES 121-122

**Example usage:**

.. GENERATED FROM PYTHON SOURCE LINES 122-131

.. code-block:: Python


    module = ScipyConv2d(3, 3)
    print("Filter and bias: ", list(module.parameters()))
    input = torch.randn(10, 10, requires_grad=True)
    output = module(input)
    print("Output from the convolution: ", output)
    output.backward(torch.randn(8, 8))
    print("Gradient for the input map: ", input.grad)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Filter and bias:  [Parameter containing:
    tensor([[ 1.2712, -1.5692, -0.1529],
            [ 0.1776, -0.9575, -1.0011],
            [ 1.1684, -0.4074,  0.7260]], requires_grad=True), Parameter containing:
    tensor([[1.6449]], requires_grad=True)]
    Output from the convolution:  tensor([[ 7.0464,  1.2547, -1.7851,  2.5481,  2.6024, -0.6286,  3.1105,  1.8177],
            [ 5.2627,  3.5132,  0.9038,  3.7336, -1.9869,  0.5535,  3.1429, -0.3613],
            [ 6.9314, -1.6551,  2.4048, -3.4818,  2.9082,  1.6636, -0.1436, -1.7654],
            [ 0.2541, -1.3050,  3.9878,  4.3217,  4.7437,  9.9691,  5.4479, -0.5548],
            [ 3.8337, -0.9995,  1.6709,  1.8141,  1.1270, -4.0174,  6.5402,  1.7882],
            [ 3.6330,  1.0630, -3.5322, -1.4698,  1.3885,  0.2308,  5.3706, -0.7348],
            [ 2.0590,  2.1167, -3.1882,  4.3703,  3.7811, -2.2196,  6.5018, -6.0961],
            [ 1.2985,  2.7692,  1.5733,  4.0564,  5.4946, -0.1341,  2.5542, -1.6332]],
           grad_fn=<ScipyConv2dFunctionBackward>)
    Gradient for the input map:  tensor([[-1.0328,  3.8654, -2.0869, -1.8076,  0.5531,  0.4241, -1.8478, -1.4823,
              3.0688,  0.2893],
            [ 0.9342,  3.1709, -4.5792, -3.2670, -0.3319, -1.1264, -2.8152,  0.1528,
              3.5661,  1.9893],
            [-1.2351,  3.3754, -2.6102, -5.6012,  0.7652,  2.4318, -5.2662, -3.9552,
              4.2824, -0.5024],
            [ 3.4415,  0.8155, -1.3480,  1.0691, -0.0619,  4.2241, -4.7261,  0.6002,
              0.4157,  1.1114],
            [ 0.3521, -1.8786, -2.0269, -0.8319,  2.0713,  3.0223, -1.2023, -1.1475,
              2.6439, -1.6384],
            [ 0.0835,  3.1215, -2.2780,  4.0195,  0.6943, -1.2061, -4.1805, -1.7473,
             -0.1907,  0.9668],
            [-0.6752,  1.7190,  3.5836, -1.0329,  0.9635, -0.4696,  1.9104, -1.9483,
              0.9670,  0.7403],
            [-0.6327,  1.0714, -3.7333,  0.8210, -3.2143,  1.2998,  2.2895, -2.8849,
             -1.9612, -1.2178],
            [-0.4490, -1.4397, -0.4232, -1.6345,  0.3265,  0.2086,  1.5771, -1.5673,
             -0.7963, -0.3355],
            [ 1.4387,  0.5645,  0.4757,  1.1050, -0.8722,  1.0496,  0.4867,  0.6236,
              0.4535,  0.4180]])




.. GENERATED FROM PYTHON SOURCE LINES 132-133

**Check the gradients:**

.. GENERATED FROM PYTHON SOURCE LINES 133-141

.. code-block:: Python


    from torch.autograd.gradcheck import gradcheck

    moduleConv = ScipyConv2d(3, 3)

    input = [torch.randn(20, 20, dtype=torch.double, requires_grad=True)]
    test = gradcheck(moduleConv, input, eps=1e-6, atol=1e-4)
    print("Are the gradients correct: ", test)




.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Are the gradients correct:  True





.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 0.602 seconds)


.. _sphx_glr_download_advanced_numpy_extensions_tutorial.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: numpy_extensions_tutorial.ipynb <numpy_extensions_tutorial.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: numpy_extensions_tutorial.py <numpy_extensions_tutorial.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: numpy_extensions_tutorial.zip <numpy_extensions_tutorial.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
