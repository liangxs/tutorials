

.. _sphx_glr_beginner_nlp:

Deep Learning for NLP with Pytorch
----------------------------------

These tutorials will walk you through the key ideas of deep learning
programming using Pytorch. Many of the concepts (such as the computation
graph abstraction and autograd) are not unique to Pytorch and are
relevant to any deep learning toolkit out there.

They are focused specifically on NLP for people who have never written
code in any deep learning framework (e.g, TensorFlow,Theano, Keras, DyNet).
The tutorials assumes working knowledge of core NLP problems: part-of-speech
tagging, language modeling, etc. It also assumes familiarity with neural
networks at the level of an intro AI class (such as one from the Russel and
Norvig book). Usually, these courses cover the basic backpropagation algorithm
on feed-forward neural networks, and make the point that they are chains of
compositions of linearities and non-linearities. This tutorial aims to get
you started writing deep learning code, given you have this prerequisite
knowledge.

Note these tutorials are about *models*, not data. For all of the models,
a few test examples are created with small dimensionality so you can see how
the weights change as it trains. If you have some real data you want to
try, you should be able to rip out any of the models from this notebook
and use them on it.

1. pytorch_tutorial.py
	Introduction to PyTorch
	https://pytorch.org/tutorials/beginner/nlp/pytorch_tutorial.html

2. deep_learning_tutorial.py
	Deep Learning with PyTorch
	https://pytorch.org/tutorials/beginner/nlp/deep_learning_tutorial.html

3. word_embeddings_tutorial.py
	Word Embeddings: Encoding Lexical Semantics
	https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html

4. sequence_models_tutorial.py
	Sequence Models and Long Short-Term Memory Networks
	https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html

5. advanced_tutorial.py
	Advanced: Making Dynamic Decisions and the Bi-LSTM CRF
	https://pytorch.org/tutorials/beginner/nlp/advanced_tutorial.html



.. raw:: html

    <div class="sphx-glr-thumbnails">

.. thumbnail-parent-div-open

.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="Introduction to Torch&#x27;s tensor library ======================================">

.. only:: html

  .. image:: /beginner/nlp/images/thumb/sphx_glr_pytorch_tutorial_thumb.png
    :alt:

  :ref:`sphx_glr_beginner_nlp_pytorch_tutorial.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">Introduction to PyTorch</div>
    </div>


.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="At this point, we have seen various feed-forward networks. That is, there is no state maintained by the network at all. This might not be the behavior we want. Sequence models are central to NLP: they are models where there is some sort of dependence through time between your inputs. The classical example of a sequence model is the Hidden Markov Model for part-of-speech tagging. Another example is the conditional random field.">

.. only:: html

  .. image:: /beginner/nlp/images/thumb/sphx_glr_sequence_models_tutorial_thumb.png
    :alt:

  :ref:`sphx_glr_beginner_nlp_sequence_models_tutorial.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">Sequence Models and Long Short-Term Memory Networks</div>
    </div>


.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="Word embeddings are dense vectors of real numbers, one per word in your vocabulary. In NLP, it is almost always the case that your features are words! But how should you represent a word in a computer? You could store its ascii character representation, but that only tells you what the word is, it doesn&#x27;t say much about what it means (you might be able to derive its part of speech from its affixes, or properties from its capitalization, but not much). Even more, in what sense could you combine these representations? We often want dense outputs from our neural networks, where the inputs are |V| dimensional, where V is our vocabulary, but often the outputs are only a few dimensional (if we are only predicting a handful of labels, for instance). How do we get from a massive dimensional space to a smaller dimensional space?">

.. only:: html

  .. image:: /beginner/nlp/images/thumb/sphx_glr_word_embeddings_tutorial_thumb.png
    :alt:

  :ref:`sphx_glr_beginner_nlp_word_embeddings_tutorial.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">Word Embeddings: Encoding Lexical Semantics</div>
    </div>


.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="Deep Learning Building Blocks: Affine maps, non-linearities and objectives ==========================================================================">

.. only:: html

  .. image:: /beginner/nlp/images/thumb/sphx_glr_deep_learning_tutorial_thumb.png
    :alt:

  :ref:`sphx_glr_beginner_nlp_deep_learning_tutorial.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">Deep Learning with PyTorch</div>
    </div>


.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="Dynamic versus Static Deep Learning Toolkits --------------------------------------------">

.. only:: html

  .. image:: /beginner/nlp/images/thumb/sphx_glr_advanced_tutorial_thumb.png
    :alt:

  :ref:`sphx_glr_beginner_nlp_advanced_tutorial.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">Advanced: Making Dynamic Decisions and the Bi-LSTM CRF</div>
    </div>


.. thumbnail-parent-div-close

.. raw:: html

    </div>


.. toctree::
   :hidden:

   /beginner/nlp/pytorch_tutorial
   /beginner/nlp/sequence_models_tutorial
   /beginner/nlp/word_embeddings_tutorial
   /beginner/nlp/deep_learning_tutorial
   /beginner/nlp/advanced_tutorial

